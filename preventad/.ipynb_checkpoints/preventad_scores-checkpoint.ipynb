{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Network Stacks for the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to investigate the variance in the preprocessed data. I will investigate the different metrics regarding std and binarized count\n",
    "1. Generate 4D stacks of all subjects for a given template network so I can flip through them\n",
    "2. Generate mean and std maps across all subjects for all networks\n",
    "3. Make a 4D stack of the template networks for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import multiprocessing as mp\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from brainbox import tools as to\n",
    "from  __builtin__ import any as b_any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_files(in_path, ext, targets, template='(?<=\\d{2})\\d{5}', sub=False):\n",
    "    \"\"\"\n",
    "    Finds matching files with extension ext and returns them in\n",
    "    the order of the targets list given as argument\n",
    "    Returns a dictionary identical to what I was using before\n",
    "    Also drops duplicates\n",
    "    \"\"\"\n",
    "    # Go through each directory and see if I can find the subjects I am looking\n",
    "    # for\n",
    "    ext = '*{}'.format(ext)\n",
    "    out_dict = {key: [] for key in ['sub_name', 'dir', 'path']}\n",
    "   \n",
    "    if not sub:\n",
    "        sub_dirs = [d for d in os.walk(in_path).next()[1]]\n",
    "        print(sub_dirs)\n",
    "        for sub_dir in sub_dirs:\n",
    "            print('heyho')\n",
    "            tmp_dir = os.path.join(in_path, sub_dir)\n",
    "            in_files = glob.glob(os.path.join(tmp_dir, ext))\n",
    "            tmp_dict = dict()\n",
    "\n",
    "            # Get the files that we have\n",
    "            matches = [x for x in targets if b_any(str(x) in t for t in in_files)]\n",
    "\n",
    "            for in_file in in_files:\n",
    "                sub_name = os.path.basename(in_file.split('.')[0])\n",
    "                sub_id = re.search(r'{}'.format(template), sub_name).group()\n",
    "                if sub_id in tmp_dict.keys():\n",
    "                    # This is a duplicate\n",
    "                    continue\n",
    "                tmp_dict[sub_id] = (sub_name, in_file)\n",
    "\n",
    "            # Re-sort the path info\n",
    "            sort_list = list()\n",
    "            for target in matches:\n",
    "                sub_name, in_file = tmp_dict[target]\n",
    "                out_dict['sub_name'].append(sub_name)\n",
    "                out_dict['dir'].append(sub_dir)\n",
    "                out_dict['path'].append(in_file)\n",
    "    else:\n",
    "        sub_dir = sub\n",
    "        tmp_dir = os.path.join(in_path, sub_dir)\n",
    "        in_files = glob.glob(os.path.join(tmp_dir, ext))\n",
    "        tmp_dict = dict()\n",
    "\n",
    "        # Get the files that we have\n",
    "        matches = [x for x in targets if b_any(str(x) in t for t in in_files)]\n",
    "\n",
    "        for in_file in in_files:\n",
    "            sub_name = os.path.basename(in_file.split('.')[0])\n",
    "            sub_id = re.search(r'{}'.format(template), sub_name).group()\n",
    "            if sub_id in tmp_dict.keys():\n",
    "                # This is a duplicate\n",
    "                continue\n",
    "            tmp_dict[sub_id] = (sub_name, in_file)\n",
    "\n",
    "        for target in matches:\n",
    "            sub_name, in_file = tmp_dict[target]\n",
    "            out_dict['sub_name'].append(sub_name)\n",
    "            out_dict['dir'].append(sub_dir)\n",
    "            out_dict['path'].append(in_file)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "scale = 12\n",
    "run = 1\n",
    "method = 'dual_regression'\n",
    "directory = '{}_sc{}'.format(method, scale)\n",
    "in_path = '/data1/pierre_scores/rest_{}'.format(run)\n",
    "out_path = '/data1/pierre_scores/out/scores_s{}/stack_maps_{}'.format(scale, run)\n",
    "if not os.path.isdir(out_path):\n",
    "    try:\n",
    "        os.makedirs(out_path)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(out_path):\n",
    "            pass\n",
    "        else: raise\n",
    "\n",
    "ta_path = '/data1/pierre_scores/pheno/model_preventad_20141215.csv'\n",
    "ext = '.nii.gz'\n",
    "\n",
    "pheno = pd.read_csv(ta_path)\n",
    "targets = pheno['subjects'].values\n",
    "file_dict = find_files(in_path, ext, targets, template='(?<=fmri_)s\\d{6}\\S{3}', sub=directory)\n",
    "num_subs = len(file_dict['path'])\n",
    "data_subs = np.array([re.search(r'(?<=fmri_)s\\d{6}\\S{3}', sub_id).group() for sub_id in file_dict['sub_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_par(args):\n",
    "    \"\"\"\n",
    "    Wrapper function to do the loading and saving in parallel\n",
    "    \"\"\"\n",
    "    ds, num_subs, use_dict, tmp_i, out_path, net_id = args\n",
    "    \n",
    "    mean_mat = np.zeros(ds[:3] + (scale,))\n",
    "    std_mat = np.zeros(ds[:3] + (scale,))\n",
    "    \n",
    "    sub_stack = np.zeros(ds[:3] + (num_subs,))\n",
    "    for sub_id in np.arange(num_subs):\n",
    "        img = nib.load(use_dict['path'][sub_id])\n",
    "        data = img.get_data()\n",
    "        net = data[..., net_id]\n",
    "        sub_stack[..., sub_id] = net\n",
    "    # Save the network stack first\n",
    "    stack_out = nib.Nifti1Image(sub_stack, tmp_i.get_affine(), tmp_i.get_header())\n",
    "    nib.save(stack_out, os.path.join(out_path, '{}_netstack_net{}_scale_{}_run_{}.nii.gz'.format(method, net_id + 1, scale, run)))\n",
    "    # Get the mean and std\n",
    "    mean = np.mean(sub_stack, axis=3)\n",
    "    mean_mat[..., net_id] = mean\n",
    "    std = np.std(sub_stack, axis=3)\n",
    "    std_mat[..., net_id] = std\n",
    "    \n",
    "    return mean_mat, std_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running things in parallel now - for speed!\n",
      "Done with that\n"
     ]
    }
   ],
   "source": [
    "# Get a template\n",
    "tmp_i = nib.load(file_dict['path'][0])\n",
    "tmp = tmp_i.get_data()\n",
    "ds = tmp.shape\n",
    "\n",
    "# Set up the parallel processing\n",
    "p_perc = 0.9\n",
    "p_count = int(np.floor(mp.cpu_count() * p_perc))\n",
    "\n",
    "# Prepare the meta mats\n",
    "mean_mat = np.zeros(ds[:3] + (scale,))\n",
    "std_mat = np.zeros(ds[:3] + (scale,))\n",
    "arg_list = list()\n",
    "for net_id in np.arange(scale):\n",
    "    arg_list.append((ds, num_subs, file_dict, tmp_i, out_path, net_id))\n",
    "    \n",
    "# Run the stuff in parallel\n",
    "print('Running things in parallel now - for speed!')\n",
    "p = mp.Pool(p_count)\n",
    "results = p.map(run_par, arg_list)\n",
    "print('Done with that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the mean and std files to /data1/pierre_scores/out/scores_s12/stack_maps_1\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    mean_mat += result[0]\n",
    "    std_mat += result[1]\n",
    "\n",
    "# Save the mean and std maps\n",
    "print('Saving the mean and std files to {}'.format(out_path))\n",
    "mean_out = nib.Nifti1Image(mean_mat, tmp_i.get_affine(), tmp_i.get_header())\n",
    "nib.save(mean_out, os.path.join(out_path, '{}_mean_stack_scale{}_{}.nii.gz'.format(method, scale, run)))\n",
    "std_out = nib.Nifti1Image(std_mat, tmp_i.get_affine(), tmp_i.get_header())\n",
    "nib.save(std_out, os.path.join(out_path, '{}_std_stack_scale{}_run_{}.nii.gz'.format(method, scale, run)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
