{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "name = 'n308_sample'\n",
    "pheno_path = '/data1/abide/Pheno/unconstrained_2box_308_sample.csv'\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'SITE_ID', u'SUB_ID', u'DX_GROUP', u'DSM_IV_TR', u'AGE_AT_SCAN',\n",
       "       u'SEX', u'HANDEDNESS_CATEGORY', u'HANDEDNESS_SCORES', u'FIQ', u'VIQ',\n",
       "       u'PIQ', u'FIQ_TEST_TYPE', u'VIQ_TEST_TYPE', u'PIQ_TEST_TYPE',\n",
       "       u'ADI_R_SOCIAL_TOTAL_A', u'ADI_R_VERBAL_TOTAL_BV', u'ADI_RRB_TOTAL_C',\n",
       "       u'ADI_R_ONSET_TOTAL_D', u'ADI_R_RSRCH_RELIABLE', u'ADOS_MODULE',\n",
       "       u'ADOS_TOTAL', u'ADOS_COMM', u'ADOS_SOCIAL', u'ADOS_STEREO_BEHAV',\n",
       "       u'ADOS_RSRCH_RELIABLE', u'ADOS_GOTHAM_SOCAFFECT', u'ADOS_GOTHAM_RRB',\n",
       "       u'ADOS_GOTHAM_TOTAL', u'ADOS_GOTHAM_SEVERITY', u'SRS_VERSION',\n",
       "       u'SRS_RAW_TOTAL', u'SRS_AWARENESS', u'SRS_COGNITION',\n",
       "       u'SRS_COMMUNICATION', u'SRS_MOTIVATION', u'SRS_MANNERISMS',\n",
       "       u'SCQ_TOTAL', u'AQ_TOTAL', u'COMORBIDITY', u'CURRENT_MED_STATUS',\n",
       "       u'MEDICATION_NAME', u'OFF_STIMULANTS_AT_SCAN',\n",
       "       u'VINELAND_RECEPTIVE_V_SCALED', u'VINELAND_EXPRESSIVE_V_SCALED',\n",
       "       u'VINELAND_WRITTEN_V_SCALED', u'VINELAND_COMMUNICATION_STANDARD',\n",
       "       u'VINELAND_PERSONAL_V_SCALED', u'VINELAND_DOMESTIC_V_SCALED',\n",
       "       u'VINELAND_COMMUNITY_V_SCALED', u'VINELAND_DAILYLVNG_STANDARD',\n",
       "       u'VINELAND_INTERPERSONAL_V_SCALED', u'VINELAND_PLAY_V_SCALED',\n",
       "       u'VINELAND_COPING_V_SCALED', u'VINELAND_SOCIAL_STANDARD',\n",
       "       u'VINELAND_SUM_SCORES', u'VINELAND_ABC_STANDARD', u'VINELAND_INFORMANT',\n",
       "       u'WISC_IV_VCI', u'WISC_IV_PRI', u'WISC_IV_WMI', u'WISC_IV_PSI',\n",
       "       u'WISC_IV_SIM_SCALED', u'WISC_IV_VOCAB_SCALED', u'WISC_IV_INFO_SCALED',\n",
       "       u'WISC_IV_BLK_DSN_SCALED', u'WISC_IV_PIC_CON_SCALED',\n",
       "       u'WISC_IV_MATRIX_SCALED', u'WISC_IV_DIGIT_SPAN_SCALED',\n",
       "       u'WISC_IV_LET_NUM_SCALED', u'WISC_IV_CODING_SCALED',\n",
       "       u'WISC_IV_SYM_SCALED', u'EYE_STATUS_AT_SCAN', u'AGE_AT_MPRAGE', u'BMI',\n",
       "       u'frames_scrubbed ', u'frames_OK ', u'FD ', u'FD_scrubbed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pheno.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add coded variable for ratio of VIQ and PIQ\n",
    "pheno['VerbRatio'] = pheno['VIQ'] / pheno['PIQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new colormap\n",
    "cdict = {'red':   ((0.0, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'green': ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 0.0, 0.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'blue':  ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 1.0, 1.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (1.0, 0.0, 0.0))\n",
    "        }\n",
    "hotcold = LinearSegmentedColormap('hotcold', cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SCALE 7\n",
      "\n",
      "    4 findings for ADOS_TOTAL ([array([2, 1]), array([2, 2]), array([2, 3]), array([5, 2])])\n",
      "\n",
      "    4 findings for VIQ ([array([5, 2]), array([5, 3]), array([6, 2]), array([6, 4])])\n",
      "\n",
      "    9 findings for DX_GROUP ([array([2, 0]), array([2, 1]), array([2, 2]), array([2, 3]), array([3, 1]), array([3, 2]), array([4, 0]), array([4, 2]), array([6, 4])])\n",
      "\n",
      "    0 findings for VINELAND_ABC_STANDARD\n",
      "\n",
      "    0 findings for SRS_RAW_TOTAL\n",
      "\n",
      "    1 findings for ADOS_SOCIAL ([array([2, 1])])\n",
      "\n",
      "    4 findings for ADOS_COMM ([array([2, 0]), array([2, 1]), array([2, 3]), array([5, 1])])\n",
      "\n",
      "    0 findings for ADOS_STEREO_BEHAV\n",
      "\n",
      "    0 findings for VerbRatio\n",
      "\n",
      "    0 findings for ADI_R_VERBAL_TOTAL_BV\n",
      "\n",
      "    0 findings for ADI_R_SOCIAL_TOTAL_A\n",
      "\n",
      "    0 findings for ADI_RRB_TOTAL_C\n",
      "\n",
      "\n",
      "SCALE 12\n",
      "\n",
      "    7 findings for ADOS_TOTAL ([array([1, 0]), array([1, 4]), array([6, 1]), array([6, 2]), array([6, 4]), array([7, 2]), array([7, 3])])\n",
      "\n",
      "    7 findings for VIQ ([array([1, 0]), array([1, 4]), array([6, 1]), array([6, 4]), array([7, 2]), array([7, 3]), array([10,  1])])\n",
      "\n",
      "    11 findings for DX_GROUP ([array([0, 0]), array([0, 3]), array([0, 4]), array([1, 0]), array([3, 3]), array([4, 1]), array([4, 2]), array([4, 3]), array([6, 1]), array([6, 2]), array([6, 4])])\n",
      "\n",
      "    0 findings for VINELAND_ABC_STANDARD\n",
      "\n",
      "    2 findings for SRS_RAW_TOTAL ([array([0, 3]), array([4, 3])])\n",
      "\n",
      "    7 findings for ADOS_SOCIAL ([array([1, 0]), array([1, 4]), array([6, 1]), array([6, 2]), array([6, 4]), array([7, 2]), array([7, 3])])\n",
      "\n",
      "    6 findings for ADOS_COMM ([array([1, 0]), array([1, 4]), array([6, 1]), array([6, 2]), array([6, 4]), array([7, 3])])\n",
      "\n",
      "    0 findings for ADOS_STEREO_BEHAV\n",
      "\n",
      "    0 findings for VerbRatio\n",
      "\n",
      "    0 findings for ADI_R_VERBAL_TOTAL_BV\n",
      "\n",
      "    0 findings for ADI_R_SOCIAL_TOTAL_A\n",
      "\n",
      "    0 findings for ADI_RRB_TOTAL_C\n",
      "\n",
      "\n",
      "SCALE 20\n",
      "\n",
      "    10 findings for ADOS_TOTAL ([array([3, 4]), array([4, 0]), array([7, 0]), array([7, 1]), array([7, 2]), array([10,  0]), array([10,  1]), array([10,  3]), array([10,  4]), array([16,  4])])\n",
      "\n",
      "    8 findings for VIQ ([array([3, 2]), array([4, 0]), array([4, 1]), array([4, 3]), array([7, 0]), array([7, 1]), array([7, 2]), array([16,  3])])\n",
      "\n",
      "    19 findings for DX_GROUP ([array([0, 0]), array([0, 2]), array([0, 3]), array([1, 2]), array([4, 0]), array([4, 4]), array([7, 0]), array([7, 1]), array([7, 2]), array([10,  0]), array([10,  1]), array([10,  3]), array([10,  4]), array([11,  1]), array([11,  4]), array([14,  0]), array([14,  2]), array([14,  3]), array([17,  3])])\n",
      "\n",
      "    1 findings for VINELAND_ABC_STANDARD ([array([9, 3])])\n",
      "\n",
      "    0 findings for SRS_RAW_TOTAL\n",
      "\n",
      "    7 findings for ADOS_SOCIAL ([array([7, 0]), array([7, 1]), array([7, 2]), array([10,  0]), array([10,  1]), array([10,  3]), array([10,  4])])\n",
      "\n",
      "    11 findings for ADOS_COMM ([array([4, 0]), array([4, 4]), array([7, 0]), array([7, 1]), array([7, 2]), array([10,  0]), array([10,  1]), array([10,  3]), array([10,  4]), array([15,  4]), array([16,  4])])\n",
      "\n",
      "    0 findings for ADOS_STEREO_BEHAV\n",
      "\n",
      "    1 findings for VerbRatio ([array([16,  0])])\n",
      "\n",
      "    0 findings for ADI_R_VERBAL_TOTAL_BV\n",
      "\n",
      "    0 findings for ADI_R_SOCIAL_TOTAL_A\n",
      "\n",
      "    0 findings for ADI_RRB_TOTAL_C\n"
     ]
    }
   ],
   "source": [
    "scales = [7,12,20]\n",
    "for scale in scales:\n",
    "    print('\\n\\nSCALE {}'.format(scale))\n",
    "    # Scale stuff\n",
    "    prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "    netstack_path = '/data1/subtypes/serial_preps/netstack_demeaned_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "    netraw_path = '/data1/subtypes/serial_preps/netstack_raw_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "    corrmat_path = '/data1/subtypes/serial_preps/correlation_matrix_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "    \n",
    "    # Get the prior\n",
    "    p_img = nib.load(prior_path)\n",
    "    prior = p_img.get_data()\n",
    "    \n",
    "    # Turn the priors into an image\n",
    "    prior = nib.load(prior_path)\n",
    "    prior_data = prior.get_data()\n",
    "    prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "    for sc_id in range(scale):\n",
    "        tmp = np.zeros_like(prior_data)\n",
    "        tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "        prior_temp[..., sc_id] = tmp\n",
    "    prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    \n",
    "    # Load the serialized netstack\n",
    "    netstack = np.load(netstack_path)\n",
    "    corr_mat = np.load(corrmat_path)\n",
    "    \n",
    "    subtypes = 5\n",
    "\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "    \n",
    "    # Make the grand average\n",
    "    gdavg = np.zeros(mask.shape + (scale,))\n",
    "\n",
    "    scale = netstack.shape[0]\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "    cov_list = ['ADOS_TOTAL', 'VIQ', 'DX_GROUP',\n",
    "                'VINELAND_ABC_STANDARD', 'SRS_RAW_TOTAL', \n",
    "                'ADOS_SOCIAL', 'ADOS_COMM', 'ADOS_STEREO_BEHAV', 'VerbRatio',\n",
    "               'ADI_R_VERBAL_TOTAL_BV', 'ADI_R_SOCIAL_TOTAL_A', 'ADI_RRB_TOTAL_C']\n",
    "            \n",
    "    for cov in cov_list:\n",
    "        cov_index = pd.notnull(pheno.replace(-9999, np.nan)[cov])\n",
    "        cov_pheno = pheno[cov_index]\n",
    "        # Generate the model matrix\n",
    "        factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "        # Make dummy variables for the site factor\n",
    "        site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "        # Turn the first site into the intercept\n",
    "        site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "        site_factor['INTERCEPT'] = 1\n",
    "        # Get the other variables\n",
    "        other_factors = cov_pheno.ix[:,factors]\n",
    "        # Turn diagnosis into [0,1] vector\n",
    "        #other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "        # Demean age\n",
    "        other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "        # Demean the covariate\n",
    "        other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "        # Put them back together\n",
    "        glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "        cov_weight = weight_store[..., cov_index.values]\n",
    "        res_store = list()\n",
    "        pval_store = np.zeros((scale, subtypes))\n",
    "        for net_id in range(scale):\n",
    "            res_list = list()\n",
    "            # Loop through the subtypes\n",
    "            for s_id in range(subtypes):\n",
    "                model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "                results = model.fit()\n",
    "                # Save the p-values\n",
    "                pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "                res_list.append(results)\n",
    "            res_store.append(res_list)\n",
    "        # Now look at the mask of p-values passing FDR Correction\n",
    "        pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "        pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        # pcorr_vec = sts.multicomp.fdrcorrection0(pval_vec, 0.05)\n",
    "        # Find the hits\n",
    "        if np.sum(pcorr_vec[0]) > 0:\n",
    "            pcorr_store = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "            hits = np.argwhere(pcorr_store!=0)\n",
    "            print('\\n    {} findings for {} ({})'.format(np.sum(pcorr_vec[0]), cov, list(hits)))\n",
    "        else:\n",
    "            print('\\n    {} findings for {}'.format(np.sum(pcorr_vec[0]), cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
